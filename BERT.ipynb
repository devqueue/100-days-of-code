{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1tDh3GV0Ip7b3Z7aZ7T8csmeYsOE0i3xQ",
      "authorship_tag": "ABX9TyOP926LZOg6YuJ+GJh2DwnT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devqueue/100-days-of-code/blob/master/BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entity recognition using BERT"
      ],
      "metadata": {
        "id": "5T5_qHkmx5ER"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVnBrboWxFIv",
        "outputId": "9630fa57-53c0-44e4-f413-ede546abd231"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'bert'...\n",
            "remote: Enumerating objects: 28, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 28 (delta 10), reused 25 (delta 7), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (28/28), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/devqueue/bert.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd bert\n",
        "!git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axb6VvDgVKty",
        "outputId": "9f6f36b8-f117-4c51-c442-81620517f50a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/bert\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 4 (delta 3), reused 4 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (4/4), done.\n",
            "From https://github.com/devqueue/bert\n",
            "   20c1b85..1b91a91  main       -> origin/main\n",
            "Updating 20c1b85..1b91a91\n",
            "Fast-forward\n",
            " src/predict.py | 2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " 1 file changed, 1 insertion(+), 1 deletion(-)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir /content/input\n",
        "%cd /content/input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpjQZANKyBiz",
        "outputId": "94a0e117-90c7-4ff6-8563-457f9fa555c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://huggingface.co/bert-base-uncased\n",
        "%cd bert-base-uncased\n",
        "!git lfs install\n",
        "!git lfs pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXERC36Uy4tv",
        "outputId": "a7ffd7cb-8696-4a11-83e7-771eead8b8df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'bert-base-uncased'...\n",
            "remote: Enumerating objects: 55, done.\u001b[K\n",
            "remote: Counting objects: 100% (55/55), done.\u001b[K\n",
            "remote: Compressing objects: 100% (53/53), done.\u001b[K\n",
            "remote: Total 55 (delta 20), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (55/55), done.\n",
            "/content/input/bert-base-uncased\n",
            "Updated git hooks.\n",
            "Git LFS initialized.\n",
            "Git LFS: (4 of 4 files) 1.81 GB / 1.81 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmB_lgnW_W6H",
        "outputId": "eea3c203-c00d-4730-f258-6002678341fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config.json\t    pytorch_model.bin  tf_model.h5\t      vocab.txt\n",
            "flax_model.msgpack  README.md\t       tokenizer_config.json\n",
            "LICENSE\t\t    rust_model.ot      tokenizer.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gt5DjUbGy_nl",
        "outputId": "46b359c8-c3d2-43df-f792-b341fba7edf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbNJiMk21Jma",
        "outputId": "0ce07139-46f2-4c44-c824-7503823954f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert  drive  input  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the dataset and requirements"
      ],
      "metadata": {
        "id": "5WYmnurP3jXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/Bert/ner_dataset.csv /content/input/ner_dataset.csv\n",
        "!cp /content/drive/MyDrive/Bert/ner.csv /content/input/ner.csv"
      ],
      "metadata": {
        "id": "8AafVAWP1KTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the model if exists\n",
        "!cp /content/drive/MyDrive/Bert/pytorch_model.bin /content/input/bert-base-uncased/pytorch_model.bin\n",
        "!cp /content/drive/MyDrive/Bert/meta.bin meta.bin"
      ],
      "metadata": {
        "id": "j5GIhYtL5WJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch tqdm joblib pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tqGVEpo9c6H",
        "outputId": "c7112371-edc2-4a1c-9ed6-414f36872661"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.22.1-py3-none-any.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.9.0\n",
            "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 67.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 37.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.9.1 tokenizers-0.12.1 transformers-4.22.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "d6GxssnU4sre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/bert/src/train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xaVuRIR-s96",
        "outputId": "7864a3a6-3776-4998-c494-1f103e6c04ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Some weights of the model checkpoint at /content/input/bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "100% 1349/1349 [14:51<00:00,  1.51it/s]\n",
            "100% 600/600 [00:40<00:00, 14.90it/s]\n",
            "Train Loss = 0.2261069800685387 Valid Loss = 0.10447169706535836\n",
            "100% 1349/1349 [15:02<00:00,  1.49it/s]\n",
            "100% 600/600 [00:40<00:00, 14.88it/s]\n",
            "Train Loss = 0.09350278319651503 Valid Loss = 0.09385780700637648\n",
            "100% 1349/1349 [15:02<00:00,  1.50it/s]\n",
            "100% 600/600 [00:40<00:00, 14.89it/s]\n",
            "Train Loss = 0.07394081305262334 Valid Loss = 0.09235611790946374\n",
            "100% 1349/1349 [15:02<00:00,  1.49it/s]\n",
            "100% 600/600 [00:40<00:00, 14.87it/s]\n",
            "Train Loss = 0.05996733448096034 Valid Loss = 0.09499832466244698\n",
            "100% 1349/1349 [15:02<00:00,  1.50it/s]\n",
            "100% 600/600 [00:40<00:00, 14.89it/s]\n",
            "Train Loss = 0.0492089987919563 Valid Loss = 0.09874599466721216\n",
            "100% 1349/1349 [15:02<00:00,  1.50it/s]\n",
            "100% 600/600 [00:40<00:00, 14.88it/s]\n",
            "Train Loss = 0.04066800965124629 Valid Loss = 0.10340393267416706\n",
            "100% 1349/1349 [15:02<00:00,  1.50it/s]\n",
            "100% 600/600 [00:40<00:00, 14.87it/s]\n",
            "Train Loss = 0.03396206545202356 Valid Loss = 0.10988284880528226\n",
            "100% 1349/1349 [15:02<00:00,  1.50it/s]\n",
            "100% 600/600 [00:40<00:00, 14.87it/s]\n",
            "Train Loss = 0.028715720826178636 Valid Loss = 0.11286396867052341\n",
            "100% 1349/1349 [15:02<00:00,  1.50it/s]\n",
            "100% 600/600 [00:40<00:00, 14.90it/s]\n",
            "Train Loss = 0.025230042794836705 Valid Loss = 0.1175487602998813\n",
            "100% 1349/1349 [15:02<00:00,  1.50it/s]\n",
            "100% 600/600 [00:40<00:00, 14.86it/s]\n",
            "Train Loss = 0.022862424095022493 Valid Loss = 0.11767331213729146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# upload the model\n",
        "!cp /content/input/bert-base-uncased/pytorch_model.bin /content/drive/MyDrive/Bert/pytorch_model.bin\n",
        "!cp meta.bin /content/drive/MyDrive/Bert/meta.bin"
      ],
      "metadata": {
        "id": "TaxZ8Y0QLF7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict"
      ],
      "metadata": {
        "id": "9826OLVO5Sky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsXBc9j4VeP_",
        "outputId": "5cc08548-e0e2-451e-f0bc-628a3d21c4b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/bert/src/predict.py \"i am traveling from banglore to delhi\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bV_KCwxm_4BQ",
        "outputId": "54fdc9fc-de06-4554-88f1-6d2c87fc1f5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator LabelEncoder from version 1.1.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  UserWarning,\n",
            "['i', 'am', 'traveling', 'from', 'banglore', 'to', 'delhi']\n",
            "[101, 1045, 2572, 7118, 2013, 9748, 20186, 2000, 6768, 102]\n",
            "Some weights of the model checkpoint at /content/input/bert-base-uncased were not used when initializing BertModel: ['out_pos.bias', 'out_pos.weight', 'out_tag.weight', 'out_tag.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "['B-art' 'O' 'O' 'O' 'O' 'B-geo' 'B-geo' 'O' 'B-geo' 'B-art']\n",
            "['$' 'PRP' 'VBP' 'VBG' 'IN' 'NNP' 'NNP' 'TO' 'NNP' '$']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/bert/src/predict.py \"My name is james\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ft9KZ88i6w1n",
        "outputId": "b1fc7e28-2197-4e51-8faa-5360274f230e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator LabelEncoder from version 1.1.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  UserWarning,\n",
            "['My', 'name', 'is', 'james']\n",
            "[101, 2026, 2171, 2003, 2508, 102]\n",
            "Some weights of the model checkpoint at /content/input/bert-base-uncased were not used when initializing BertModel: ['out_tag.weight', 'out_pos.bias', 'out_tag.bias', 'out_pos.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "['B-art' 'O' 'O' 'O' 'B-per' 'B-art']\n",
            "['$' 'PRP$' 'NN' 'VBZ' 'NNP' '$']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!python /content/bert/src/predict.py \"My name is james clear\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXropVtnA2om",
        "outputId": "2fb04dc2-82a0-4f7d-97bc-f294dbcf7239"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator LabelEncoder from version 1.1.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  UserWarning,\n",
            "['My', 'name', 'is', 'james', 'clear']\n",
            "[101, 2026, 2171, 2003, 2508, 3154, 102]\n",
            "Some weights of the model checkpoint at /content/input/bert-base-uncased were not used when initializing BertModel: ['out_tag.bias', 'out_pos.weight', 'out_tag.weight', 'out_pos.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "['B-art' 'O' 'O' 'O' 'B-per' 'O' 'B-art']\n",
            "['$' 'PRP$' 'NN' 'VBZ' 'NNP' 'NNP' '$']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/bert/src/predict.py \"My name is james bond\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtqXzg6tCNe2",
        "outputId": "2d8a2c06-e4c0-4b3f-b55f-759d123ad721"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator LabelEncoder from version 1.1.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  UserWarning,\n",
            "['My', 'name', 'is', 'james', 'bond']\n",
            "[101, 2026, 2171, 2003, 2508, 5416, 102]\n",
            "Some weights of the model checkpoint at /content/input/bert-base-uncased were not used when initializing BertModel: ['out_tag.bias', 'out_pos.weight', 'out_pos.bias', 'out_tag.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "['B-art' 'O' 'O' 'O' 'B-per' 'I-per' 'B-art']\n",
            "['$' 'PRP$' 'NN' 'VBZ' 'NNP' 'NNP' '$']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/bert/src/predict.py 'The base radius should be a minimum of 0.25 X thickness. Bosses can be strengthened by incorporating gussets at the base or by using connecting ribs attaching to nearby walls.'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LS9sFK9tip6u",
        "outputId": "738d1d11-a793-40bb-e5ae-887d8b0dc3a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
            "Moving 0 files to the new cache system\n",
            "\r0it [00:00, ?it/s]\r0it [00:00, ?it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator LabelEncoder from version 1.1.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  UserWarning,\n",
            "['The', 'base', 'radius', 'should', 'be', 'a', 'minimum', 'of', '0.25', 'X', 'thick-', 'ness.', 'Bosses', 'can', 'be', 'strengthened', 'by', 'incorporating', 'gussets', 'at', 'the', 'base', 'or', 'by', 'using', 'connecting', 'ribs', 'attaching', 'to', 'nearby', 'walls.']\n",
            "[101, 1996, 2918, 12177, 2323, 2022, 1037, 6263, 1997, 1014, 1012, 2423, 1060, 4317, 1011, 23384, 1012, 23029, 2064, 2022, 13949, 2011, 13543, 12670, 13462, 2015, 2012, 1996, 2918, 2030, 2011, 2478, 7176, 10335, 22476, 2075, 2000, 3518, 3681, 1012, 102]\n",
            "Some weights of the model checkpoint at /content/input/bert-base-uncased were not used when initializing BertModel: ['out_tag.weight', 'out_tag.bias', 'out_pos.bias', 'out_pos.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/bert/src/predict.py\", line 36, in <module>\n",
            "    model.load_state_dict(torch.load(config.BASE_MODEL_PATH + '/' + config.MODEL_PATH))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 712, in load\n",
            "    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 1049, in _load\n",
            "    result = unpickler.load()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 1019, in persistent_load\n",
            "    load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 1001, in load_tensor\n",
            "    wrap_storage=restore_location(storage, location),\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 175, in default_restore_location\n",
            "    result = fn(storage, location)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 152, in _cuda_deserialize\n",
            "    device = validate_cuda_device(location)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 136, in validate_cuda_device\n",
            "    raise RuntimeError('Attempting to deserialize object on a CUDA '\n",
            "RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "NER = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "V-RpXENcXHfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text=\"The Indian Space Research Organisation or is the national space agency of India, headquartered in Bengaluru. It operates under Department of Space which is directly overseen by the Prime Minister of India while Chairman of ISRO acts as executive of DOS as well.\"\n",
        "# text1= NER(raw_tex.,t)\n",
        "displacy.render(text1,style=\"ent\",jupyter=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "P1UA-IuGXOb1",
        "outputId": "95daa5a0-0eb8-440a-ddda-d072c754300e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    The Indian Space Research Organisation\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " or is the national space agency of \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    India\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ", headquartered in \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bengaluru\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ". It operates under \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Department of Space\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " which is directly overseen by the Prime Minister of \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    India\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " while Chairman of ISRO acts as executive of DOS as well.</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw = 'what is the date today'\n",
        "text= NER(raw)\n",
        "displacy.render(text,style=\"ent\",jupyter=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "DIG7Utv_Yl7V",
        "outputId": "a4ae5b62-f861-4920-f4ba-fcbd739932ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">what is the date \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    today\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              "</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw = '''The base radius should be a minimum of 0.25 X thick- ness. Bosses can be strengthened by incorporating gussets at the base or by using connecting ribs attaching to nearby walls.'''\n",
        "\n",
        "text= NER(raw)\n",
        "displacy.render(text,style=\"ent\",jupyter=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "KnGdKRTFhADn",
        "outputId": "d02e74b9-b39f-41c2-b8fd-5eca1e845193"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">The base radius should be a minimum of \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    0.25\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
              "</mark>\n",
              " X thick-</br>ness. Bosses can be strengthened by incorporating gus-</br>sets at the base or by using connecting ribs attaching to</br>nearby walls.</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f5snnt0ZYmZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qthe8EzJYm-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BfpJ3ZPfY8AU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}